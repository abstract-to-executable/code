env: "BlockStackTrajectory-v0"
device: "cuda"
env_cfg:
  early_success: False
  task_agnostic: False
  reward_type: "traj_dense"
  # path to file with trajectory ids
  trajectories: [0]
  trajectories_dataset: "datasets/blockstack_v2/dataset_resampled.pkl"
  max_trajectory_skip_steps: 15 # not using?
  give_traj_id: True
  max_rot_stray_dist: 0.25 # for an angle smaller than 60 deg
  max_world_state_stray_dist: 0.03 # half block size, for diff in blocks
  max_coord_stray_dist: 0.03 # diff between  coords of teacher / student panda hands
  max_finger_stray_dist: 0.015 # diff between t/s grippers
  max_hand_and_block_stray_dist: 0.06 # If using this, no longer need max_world/max_coord_stry_dist
  agent_diff_ratio: 0.1 # not using
  max_stray_dist: 0.04 # not using
  robot_type: 'Arm'


exp_cfg:
  algo: ppo
  seed: 0
  n_envs: 8

  gae_lambda: 0.95
  target_kl: 0.2
  log_std_scale: -0.5
  pi_lr: 3e-4
  vf_lr: 3e-4

  accumulate_grads: False
  #ppo configs
  epochs: 10000
  critic_warmup_epochs: 0
  update_iters: 3
  steps_per_epoch: 10000
  batch_size: 512
  eval_freq: 50
  eval_save_video: True
  max_ep_len: 125



  dapg: False

logging_cfg:
  exp_name: blockstack_v2_rl/mlp_0
  wandb: False
  tensorboard: True
  log_freq: 1

model_cfg:
  type: "LSTM"
  pretrained_actor_weights: None
  pretrained_critic_weights: None

  state_dims: 38
  act_dims: 4
  teacher_dims: 12

  max_time_steps: 1
  # below should also be merged into dataset_cfgs
  max_student_length: 300
  max_teacher_length: 32
  trajectory_sample_skip_steps: 8
  # equivalent to positional embeddings
  timestep_embeddings: False
  # whether to include past student actions into the student stack frames fed into transformer
  use_past_actions: False
  # whether to use layer normalization after the initial embedding layers of student/teacher states and student actions
  embed_layer_norm: True

  # translation model specific configs
  stack_size: 5
  state_embedding_hidden_sizes: (64, )
  state_embedding_activation: 'relu'
  final_mlp_hidden_sizes: (64,64)
  final_mlp_activation: 'relu'

  final_mlp_action_pred_activation: 'tanh'
  final_mlp_state_pred_activation: 'tanh'

  # final_mlp_action_pred_activation: 'identity'
  # final_mlp_state_pred_activation: 'identity'
  lstm_config:
    dropout: 0
    num_layers: 4